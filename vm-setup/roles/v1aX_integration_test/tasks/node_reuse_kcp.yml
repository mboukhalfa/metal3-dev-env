---
  - set_fact:
      NUMBER_OF_BMH_KCP: "{{ NUM_OF_MASTER_REPLICAS|int }}"
      NUMBER_OF_BMH_MD: "{{ NUM_OF_WORKER_REPLICAS|int }}"
      NUMBER_OF_ALL_BMH: "{{ NUM_OF_WORKER_REPLICAS|int + NUM_OF_MASTER_REPLICAS|int }}"
  
  - name: Scale worker down to 0.
    shell: |
        kubectl scale machinedeployment "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" --replicas=0
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
  
  - name: Wait until worker is scaled down and "{{ NUMBER_OF_BMH_MD }}" BMH is Ready.
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w ready | wc -l
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: ready_hosts
    until: ready_hosts.stdout == NUMBER_OF_BMH_MD

  - name: Get provisioned BMH names and UUIDs mapping before upgrade.
    shell: |
        kubectl get bmh -A -o json | jq '.items[]| select (.status.provisioning.state == "provisioned")
        | "metal3/"+.metadata.name+"="+"metal3://"+.metadata.uid' | cut -f2 -d\" | sort > /tmp/before_upgrade_mapping.txt
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Update maxSurge field in KCP
    shell: |
        kubectl get kubeadmcontrolplane "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" -o json | jq '.spec.rolloutStrategy.rollingUpdate.maxSurge=0' | kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Update Metal3MachineTemplate nodeReuse field to 'True'.
    shell: |
        kubectl get m3mt "{{ CLUSTER_NAME }}"-controlplane -n "{{ NAMESPACE }}" -ojson | jq '.spec.nodeReuse=true' | kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
 
  - name: Upgrade KubeadmControlPlane k8s version from "{{ KUBERNETES_VERSION }}" to "{{ UPGRADED_KUBERNETES_VERSION }}".
    shell: |
        kubectl get kubeadmcontrolplane "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" -o json | jq '.spec.version="{{ UPGRADED_KUBERNETES_VERSION }}"' | kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - pause:
      minutes: 1

  - name: Wait until "{{ NUMBER_OF_BMH_MD }}" BMH is in deprovisioning state.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" -o json | jq -r '[ .items[]
        | select (.status.provisioning.state == "deprovisioning")
        | .metadata.name ] | length'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: bmh_in_deprovisioning
    retries: 150
    delay: 10
    until: bmh_in_deprovisioning.stdout == NUMBER_OF_BMH_MD

  - name: Get the name of the deprovisioning BMH.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" -o json | jq -r '[ .items[]
        | select (.status.provisioning.state == "deprovisioning")
        | .metadata.name ] | add'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: deprovisioning_bmh_name

  - name: Wait until above deprovisioning BMH is in ready state again.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" | grep -w ready | awk '{print $1}'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: ready_bmh
    retries: 200
    delay: 2
    until: deprovisioning_bmh_name.stdout in ready_bmh.stdout_lines

  - name: Check if just deprovisioned and became ready BMH is re-used for next provisioning.
    shell: |
        kubectl get bmh -n "{{ NAMESPACE }}" | grep -w provisioning | awk '{print $1}'
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: provisioning_bmh
    retries: 150
    delay: 2
    until: deprovisioning_bmh_name.stdout in provisioning_bmh.stdout_lines

  - name: Wait until two machines become running and updated with new "{{ UPGRADED_KUBERNETES_VERSION }}" k8s version.
    shell: |
        kubectl get machines -n "{{ NAMESPACE }}" -o json | jq -r '.items[] | select (.status.phase == "Running") | select(.spec.version == "{{ UPGRADED_KUBERNETES_VERSION }}") | .status.phase' | grep -c "Running"
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: updated_machines_partly
    retries: 200
    delay: 20
    ignore_errors: yes
    until: updated_machines_partly.stdout|int > 1

  - pause:
      minutes: 5

#####UNTAINT BEGIN
  - name: Untaint all CP nodes after upgrade of two controlplane nodes
    shell: |
        kubectl taint nodes --all node-role.kubernetes.io/master-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    ignore_errors: yes

  - name: Wait until all "{{ NUMBER_OF_BMH_KCP }}" machines become running and updated with new "{{ UPGRADED_KUBERNETES_VERSION }}" k8s version.
    shell: |
        kubectl get machines -n "{{ NAMESPACE }}" -o json | jq -r '.items[] | select (.status.phase == "Running") | select(.spec.version == "{{ UPGRADED_KUBERNETES_VERSION }}") | .status.phase' | grep -c "Running"
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: updated_machines_all
    retries: 200
    delay: 20
    ignore_errors: yes
    until: updated_machines_all.stdout|int == 3

#####UNTAINT FINISH

  - name: Get BMH names and Metal3Machine provider IDs mapping after upgrade.
    shell: |
        kubectl get bmh -A -o json | jq '.items[]| select (.status.provisioning.state == "provisioned")
        | "metal3/"+.metadata.name+"="+"metal3://"+.metadata.uid' | cut -f2 -d\" | sort > /tmp/after_upgrade_mapping.txt
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Check diff of before and after upgrade mappings to make sure same BMHs' were reused in KubeadmControlPlane test scenario.
    shell: |
        diff /tmp/before_upgrade_mapping.txt /tmp/after_upgrade_mapping.txt
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: diff_mapping
    failed_when: diff_mapping.rc == 1

  - name: Clean up any KubeadmControlPlane test scenario related temp files.
    shell: rm /tmp/before_upgrade_mapping.txt /tmp/after_upgrade_mapping.txt
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Put maxSurge field in KCP back to default
    shell: |
        kubectl get kubeadmcontrolplane "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" -o json | jq '.spec.rolloutStrategy.rollingUpdate.maxSurge=1' | kubectl apply -f-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
 
  - name: Scale controlplane down to 1.
    shell: |
        kubectl scale kubeadmcontrolplane "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" --replicas=1
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - pause:
      minutes: 5

  - name: Untaint all CP nodes
    shell: |
        kubectl taint nodes --all node-role.kubernetes.io/master-
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    ignore_errors: yes

  - name: Wait until controlplane is scaled down and three  BMHs' are Ready.
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w ready | wc -l
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: all_ready_hosts
    until: all_ready_hosts.stdout == "3"

  - name: Scale worker up to "{{ NUMBER_OF_BMH_MD }}".
    shell: |
        kubectl scale machinedeployment "{{ CLUSTER_NAME }}" -n "{{ NAMESPACE }}" --replicas=1
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"

  - name: Wait until one more BMH is provisioned.
    shell: kubectl get bmh -n "{{ NAMESPACE }}" | grep -w provisioned | wc -l
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    retries: 200
    delay: 20
    register: provisioned_host
    until: provisioned_host.stdout == "2"

  - name: Wait until one more machine becomes running.
    shell: |
        kubectl get machines -n "{{ NAMESPACE }}" -o json | jq -r '.items[] | select (.status.phase == "Running") | .status.phase' | grep -c "Running"
    environment:
      KUBECONFIG: "/tmp/kubeconfig-{{ CLUSTER_NAME }}.yaml"
    register: provisioned_machine
    retries: 200
    delay: 20
    until: provisioned_machine.stdout|int == 2